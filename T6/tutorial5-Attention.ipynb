{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"../3600.jpg\" width=500 />\n",
    "\n",
    "# Tutorial 5: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Todays Agenda\n",
    "---\n",
    "- recap on sequnce modeling\n",
    "- Encoder Decoder and Latent space\n",
    "- Attention mechanisms\n",
    "    - dot product attention\n",
    "    - Additive attention\n",
    "    - self attention\n",
    "- Implementing attention\n",
    "- machine translation\n",
    "    - Encoder Decoder with GRU\n",
    "    - BLEU score\n",
    "    - Encoder Decoder with self attention for Alignment\n",
    "### second part\n",
    "- Transformers architecture\n",
    "- Positinal encoding\n",
    "- Inference with transformer\n",
    "- introduction to VIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminders and recaps\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Last tutorial we learned about RNNs:\n",
    "\n",
    "with 2 layer $l$:\n",
    "\n",
    "<center><img src=\"resources/rnn_layered.png\" width=\"1100\" /></center>\n",
    "\n",
    "Where, for each layer $l\\geq 0$ ($l_0 is input$) and \"time\" $t > 0$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{h}_t^l &= \\varphi_1\\left( \\mat{W}_{hh}^l \\vec{h}_{t-1}^l + \\mat{W}_{xh}^l \\vec{x}_t^{l-1} + \\vec{b}_h^l\\right) \\\\\n",
    "\\vec{x}_t^l &= \\varphi_2\\left(\\mat{W}_{hy}^l\\vec{h}_t^l + \\vec{b}_x^l \\right).\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and we remember, that with long sequnce as input, we need to deal with the problem of vanishing/exploading gradients.<br>\n",
    "that is why, we presented LSTM and GRU cells:\n",
    "\n",
    "<center><img src=\"resources/Network-Structure-of-RNN-LSTM-and-GRU.png\" width=\"1000\" /></center>\n",
    "\n",
    "as for the GRU:\n",
    "\n",
    "- **reset gate**:\n",
    "$$ R_t = \\sigma(X_tW_{xr} + H_{t-1}W_{hr} +b_r) \\in \\mathbb{R}^{n \\times h}$$\n",
    "- **update gate**:\n",
    "$$ Z_t = \\sigma(X_tW_{xz} + H_{t-1}W_{hz} +b_z) \\in \\mathbb{R}^{n \\times h} $$\n",
    "\n",
    "- **candidate hidden state**: \n",
    " $$ \\tilde{H}_{t} = \\text{tanh}\\left(X_t W_{xh} + (R_t \\odot H_{t-1})W_{hh} \\right) + b_h$$\n",
    "- **hidden state**:\n",
    "$$ H_t = Z_t \\odot H_{t-1} +(1-Z_t) \\odot \\tilde{H}_t. $$\n",
    "\n",
    "The main idea is to have \"soft-gates\" ($\\vec{r}_t$ and $\\vec{z}_t$) that control how much of the previous state ($\\vec{h}_{t-1}$) affects the next state $\\vec{h}_t$ relative to the proposed next state $\\tilde{\\vec{h}}_{t}$.\n",
    "\n",
    "and of course, we learned Bidirectional RNN:\n",
    "<center><img src=\"resources/bi.png\" width=\"900\" /></center>\n",
    "\n",
    "that for some tasks, we can aggregate hidden state from both begining and end of the sentence, and have a better idea of the real context we need to keep in $\\vec{h}_{t}$ (this does not effect the vanishing gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this improvments still leave us with some issues.\n",
    "\n",
    "### 1. Long inputs\n",
    "\n",
    "we saw that with CNNs, when we want a large receptive field, we need to use deeper models.\n",
    "FHD images (1080p) are 1920 x 1080 , 4K (2160p) = 3840 x 2160 and for that sizes, features across the image is hard to learn, but resnet do a [pretty good job](https://gist.github.com/samson-wang/a6073c18f2adf16e0ab5fb95b53db3e6). <br>\n",
    "\n",
    "in NLP however, RNN based models can deal with longer inputs sequnces when we use memory based cells, yet the sequnces remain couple of dozens of words.\n",
    "that means that for tasks like sentiment analysis, we would be able to read a ~100 words review. but if we want to read a book, or a wikipedia page, we will have vanishing/exploading gradients interfere, even with smarter BPTT.\n",
    "\n",
    "\n",
    "### 2. blanks\n",
    "as a hole, the RNN model help us with seq2seq or seq2one. but we didn't talk about what happen if we want to predict a word in the middle, such task from elementry school, like **\"i am ___ home\"** <br>\n",
    "this task might be very complex and can use bidirectional-RNN, but the aggregation between the context from two networks might be hard to achive.\n",
    "\n",
    "\n",
    "### 3. long-term dependencies\n",
    "one might say that we solved that with the memory based RNNs, yet soon we're going to talk about encoder-decoder models, and if you think of it, the hidden state need to encode the meaning behind the current input, as well as the dependencies in the sentence in order to propogate gradients wisely. <br>\n",
    "when we have long-term dependencies, more of the hidden state need learn the dependencies and less for understanding the meaning. here the problem is adversery while we have 2 tasks for one learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have some sort of mechanism that deals with this problems<br>\n",
    "this mechanism actually exist and and called **Attention mechanism**<br>\n",
    "\n",
    "but before we dive deep into it, let's understand some other concept called **Encoder** and **Decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder-Decoder\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To rethink of the 3rd problem, let's talk a bit about what is that hidden state $h_t$.<br>\n",
    "\n",
    "We can encode the input vector into some latent space.<br>\n",
    "\n",
    "last tutorial we already saw a similar idea using the embeding layer.\n",
    "\n",
    "Now if we want to encode all the input (sequence, image...) into some **latent space**.\n",
    "\n",
    "**latent space**, also known as a latent feature space or **embedding space**, is an embedding of a set of items within a manifold in which items which resemble each other more closely are positioned closer to one another in the latent space\n",
    "\n",
    "so if in the embeding we learned for words token, the manifold was $\\mathcal{M}: \\mathcal{R}_N -> \\mathcal{R}^E$ where N is input space and $E$ is the embeding dimention,<br>\n",
    "now, we would like to use latent space to a manifold $\\mathcal{M}: \\mathcal{R}^{Sec} -> \\mathcal{R}^{E_2}$ where $\\mathcal{R}^{Sec}$ represent any input (in NLP, think of it as we have **max len** of input, so `Sec` would be from the dim $N^{max len}$)\n",
    "\n",
    "note the diffrence, now we need to learn a mapping, not from $R^1$ but the latent space should map a huge input space, therfore should typically be bigger as well\n",
    "\n",
    "<center><img src=\"resources/ed1.PNG\" width=\"900\" /></center>\n",
    "\n",
    "<center><img src=\"resources/edSEG.png\" width=\"900\" /></center>\n",
    "\n",
    "<center><img src=\"resources/edlstm.png\" width=\"900\" /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Attention\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "originaly presented by [Bahdanau](https://arxiv.org/pdf/1409.0473.pdf),\n",
    "\n",
    "### What is Attention?\n",
    "\n",
    "The attention mechanism describes a (sort of) new type of layers for DNN's, especially in sequence tasks but not only.<br>\n",
    "\n",
    "You can find a lot of definitions, but we're going to use it in the notion of \"the decoder learn on what part of the input to focus (pay attention to)\"<br>\n",
    "\n",
    "for some reason, attention is defined with SQL notation:\n",
    "\n",
    "- **Query**: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
    "\n",
    "- **Keys**: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
    "\n",
    "- **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
    "\n",
    "- **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and output the relecance score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
    "\n",
    "\n",
    "there is another definition of `soft attention` and `hard attention`... now we will do some math, and it's a bit confusing so try to keep up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use to notation of\n",
    "- $n$ **key-value** pairs: $\\left\\{\\left(\\vec{k}_i, \\vec{v}_i\\right)\\right\\}_{i=1}^{n}$, where $\\vec{k}_i\\in\\set{R}^{d_k}$, $\\vec{v}_i\\in\\set{R}^{d_v}$\n",
    "- A **query**, $\\vec{q} \\in\\set{R}^{d_q}$ (the input)\n",
    "- **score** / **similarity** (sometimes **energy**) function between keys and queries, $f_{attn}: \\set{R}^{d_k}\\times \\set{R}^{d_q} \\mapsto \\set{R}$\n",
    "- **a** will denote Attention score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic attention, defined as we take this score function $f_{attn}$,between the **query** and each **key**, for each key.\n",
    "\n",
    "$$\n",
    "b_i = f_{attn}(\\vec{k}_i, \\vec{q}) \\in \\mathbb{R}, \\vec{b} \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "\n",
    "we compute the **attention weights** $a_i$ as:\n",
    "\n",
    "$$\n",
    "\\vec{a} = Softmax(\\vec{b})\\\\\n",
    "\\vec{a_i} = \\frac{exp(f_{attn}(\\vec{k}_i, \\vec{q}))}{\\sum_j{exp(f_{attn}(\\vec{k}_j, \\vec{q}))}}\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "`soft attention` output is\n",
    "$$\n",
    "\\vec{y} = \\sum_{i=1}^{n} a_i \\vec{v}_i\\ \\in \\set{R}^{d_v},\n",
    "$$\n",
    "\n",
    " `hard attention` output is\n",
    "\n",
    "$$\n",
    "\\vec{y} \\sim \\mathrm{Multinoulli}\\left(\\vec{a}; \\left\\{\\vec{v}_i\\right\\}_{i=1}^{n}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now there are many types of score functions $f_{attn}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One basic type of attention mechanism uses a simple **dot product** as the similarity function.\n",
    "\n",
    "Widely-used by models based on the **Transformer** architecture, we're going to focus on it later on...\n",
    "\n",
    "Assume $d_k=d_q=d$\n",
    "\n",
    "$$\n",
    "f_{attn}(\\vec{k},\\vec{q})= \\frac{\\vectr{k}\\vec{q}}{\\sqrt{d}}.\n",
    "$$\n",
    "\n",
    "Why scale by $\\sqrt{d}$ ?\n",
    "\n",
    "It's the factor at which the dot-product grows due to the dimensionality. E.g.,\n",
    "\n",
    "$$\n",
    "\\norm{\\vec{1}_d}_2 = \\norm{[1,\\dots,1]\\Tr}_2 = \\sqrt{d\\cdot 1^2} =\\sqrt{d}.\n",
    "$$\n",
    "\n",
    "This helps keep the softmax values from becoming very small when the dimension is large, and therefore helps prevent tiny gradients.\n",
    "\n",
    "Let's now deal with $m$ queries simultaneously by stacking them as rows in a matrix $\\mat{Q} \\in \\set{R}^{m\\times d}$.\n",
    "\n",
    "Similarly, we'll stack the keys and values as rows in their own matrices, $\\mat{K}\\in\\set{R}^{n\\times d}$, $\\mat{V}\\in\\set{R}^{n\\times d_v}$.\n",
    "\n",
    "Then we can compute the attention weights for all queries in parallel:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mat{B} &= \\frac{1}{\\sqrt{d}} \\mat{Q}\\mattr{K}  \\ \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{A} &= softmax({\\frac{\\mat{Q}\\mattr{K}}{\\sqrt{d}}}) \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{Y} &= \\mat{A}\\mat{V} \\ \\in\\set{R}^{m\\times d_v}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that the softmax is applied per-row, and so each row $i$ of $\\mat{A}$ contains the attention weights for the $i$th query.\n",
    "\n",
    "Also notice that in this formulation, we **input a sequence** of $m$ queries and get an **output sequence** of $m$ weighed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"resources/dotattn.PNG\" width=\"400\" /></center>\n",
    "\n",
    "`mask` is optional for specific inputs, as for some tasks, we don't want to use the same index for learning (if we want to predict the missing word for instance) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " tensor([[-0.4367,  0.7536],\n",
      "        [ 1.9926, -0.6255],\n",
      "        [ 1.0077,  0.6951]])\n",
      "K\n",
      " tensor([[-0.2703,  0.2548],\n",
      "        [ 0.7064, -0.2260],\n",
      "        [ 0.8416,  1.4440]])\n",
      "V\n",
      " tensor([[ 1.0389,  0.3326],\n",
      "        [-0.3039, -0.4371],\n",
      "        [-0.9877,  0.6341]])\n",
      "Values\n",
      " tensor([[-0.1566,  0.3197],\n",
      "        [-0.3719, -0.0015],\n",
      "        [-0.5127,  0.3289]])\n",
      "Attention\n",
      " tensor([[0.3437, 0.1968, 0.4595],\n",
      "        [0.1146, 0.5611, 0.3243],\n",
      "        [0.1528, 0.2419, 0.6053]])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multiplicative attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now assime Assume $d_k \\ne d_q$\n",
    "\n",
    "we can preform something similar to before, as \n",
    "$$\n",
    "f_{attn}(\\vec{k},\\vec{q})= {\\vectr{k}\\vec{W}\\vec{q}}; W \\in \\mathcal{R}^{d_k x d_q}\n",
    "$$\n",
    "\n",
    "note that in that case we just add one learnable layer that learn the relation between the keys and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Additive attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common type of attention mechanism uses an MLP to **learn** the similarity function $e(\\vec{k},\\vec{q})$.\n",
    "\n",
    "In this type of attention, the similarity function is \n",
    "\n",
    "$$\n",
    "f_{attn}(\\vec{k},\\vec{q}) = \\vectr{w} \\tanh(\\mat{W}_k\\vec{k} + \\mat{W}_q\\vec{q}),\n",
    "$$\n",
    "\n",
    "where $\\mat{W}_k\\in\\set{R}^{h\\times d_k}$, $\\mat{W}_q\\in\\set{R}^{h\\times d_q}$ and $\\vec{w}\\in\\set{R}^{h}$ are trainable parameters.\n",
    "\n",
    "- Notice that we're adding projected versions of the key and query and applying a 2-layer MLP.\n",
    "- Both projections and the output layer are trainable.\n",
    "\n",
    "**This Attention mechanisem is less in use, thus we're not going to focus on the visuallization and implementation**.\n",
    "if you do want to implement it, [here is an example](https://sigmoidal.io/implementing-additive-attention-in-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention refers to applying attention on a **single sequence**  $\\left\\{\\vec{x}_i\\right\\}_{i=1}^{n}$ of elements.\n",
    "\n",
    "The keys, values and queries are Computed from them with **learned** linear projections, i.e.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{q}_{i} &= \\mat{W}_{xq}\\vec{x}_{i} &\n",
    "\\vec{k}_{i} &= \\mat{W}_{xk}\\vec{x}_{i} &\n",
    "\\vec{v}_{i} &= \\mat{W}_{xv}\\vec{x}_{i} \n",
    "\\end{align*}\n",
    "$$\n",
    "This is what Transformer models do for Seq2Seq, instead of using RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the following image depicts dot product self-attention transforming the sequence $\\left\\{\\vec{x}_i\\right\\}_{i=1}^{n}$ into the sequence $\\left\\{\\vec{y}_i\\right\\}_{i=1}^{n}$.\n",
    "\n",
    "<center><img src=\"resources/self_attn_transformer.svg\" width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence Machine Translation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll translate text from German to English.\n",
    "\n",
    "The general approach using RNNs is to design a Sequence-to-sequence (**Seq2Seq**) Encoder-Decoder architecture:\n",
    "\n",
    "<center><img src=\"resources/seq2seq1.png\" width=\"1100\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The encoder processes the source sequence\n",
    "- The final encoder hidden state represents the entire source sequence\n",
    "- This representation is passed to the decoder as it's initial hidden state\n",
    "- During training the decoder is fed with the target sequence\n",
    "- Both parts are trained together, end to end\n",
    "\n",
    "We'll implement this idea roughly based on [Sutskever et al. (2014)](https://arxiv.org/abs/1409.3215), then we're going to add attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We'll use the [Multi30K](https://www.statmt.org/wmt16/multimodal-task.html) Multimodal Dataset, which contains 30K images and sentences describing them in both German and English. We'll only use the textual data.\n",
    "\n",
    "The `torchtext` package provides us with a convenient way to load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field\n",
    "\n",
    "# Common args for field objects\n",
    "field_args = dict(tokenize='spacy',\n",
    "                  init_token='<sos>',\n",
    "                  eos_token='<eos>',\n",
    "                  include_lengths=True,\n",
    "                  lower=True) \n",
    "\n",
    "# Field for processing German source\n",
    "src_field = Field(tokenizer_language=\"de_core_news_sm\", **field_args)\n",
    "\n",
    "# Field for processing English target\n",
    "tgt_field = Field(tokenizer_language=\"en_core_web_sm\", **field_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_valid, ds_test = Multi30k.splits(\n",
    "    root=data_dir, exts=('.de', '.en'), fields=(src_field, tgt_field)\n",
    ")\n",
    "\n",
    "VOCAB_MIN_FREQ = 2 # ignore very rare words to decrease vocab size\n",
    "src_field.build_vocab(ds_train, min_freq=VOCAB_MIN_FREQ)\n",
    "tgt_field.build_vocab(ds_train, min_freq=VOCAB_MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train samples:  29000\n",
      "#valid samples:  1014\n",
      "#test  samples:  1000\n"
     ]
    }
   ],
   "source": [
    "print('#train samples: ', len(ds_train))\n",
    "print('#valid samples: ', len(ds_valid))\n",
    "print('#test  samples: ', len(ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vocab size: 7854\n",
      "target vocab size: 5893\n"
     ]
    }
   ],
   "source": [
    "V_src = len(src_field.vocab)\n",
    "print(f'source vocab size: {V_src}')\n",
    "\n",
    "V_tgt = len(tgt_field.vocab)\n",
    "print(f'target vocab size: {V_tgt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample #05010:\n",
      "\tDE: junge im schnee zeigt beide daumen hoch .\n",
      "\tEN: a boy gives being in the snow two thumbs up .\n",
      "sample #08098:\n",
      "\tDE: drei typen sind dabei , backflips auszuführen .\n",
      "\tEN: three guys are in the middle of doing back flips .\n",
      "sample #11091:\n",
      "\tDE: ein kleiner hund mit einem grünen pullover und einem rucksack läuft durch den schnee .\n",
      "\tEN: a small dog wearing a green sweater and a backpack walks through snow .\n",
      "sample #23231:\n",
      "\tDE: eine junger blonder junge , der eine nummer vorne auf seinem t-shirt trägt , lehnt auf einem geländer .\n",
      "\tEN: a young blond - haired boy wearing a number on the front of him leans on a railing .\n",
      "sample #13162:\n",
      "\tDE: ein schwarzer pitbull läuft durch den dreck .\n",
      "\tEN: a black pitbull dog is running through the dirt .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for _ in range(5):\n",
    "    i = random.randrange(len(ds_train))\n",
    "    example = ds_train[i]\n",
    "    src = str.join(\" \", example.src)\n",
    "    tgt = str.join(\" \", example.trg)\n",
    "    print(f'sample #{i:05d}:\\n\\tDE: {src}\\n\\tEN: {tgt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the top 20 words in english and german and the numbers assigned for known tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE top20: ['<unk>', '<pad>', '<sos>', '<eos>', '.', 'ein', 'einem', 'in', 'eine', ',', 'und', 'mit', 'auf', 'mann', 'einer', 'der', 'frau', 'die', 'zwei', 'einen']\n",
      "\n",
      "EN top20: ['<unk>', '<pad>', '<sos>', '<eos>', 'a', '.', 'in', 'the', 'on', 'man', 'is', 'and', 'of', 'with', 'woman', ',', 'two', 'are', 'to', 'people']\n",
      "\n",
      "<unk>=0, <pad>=1\n"
     ]
    }
   ],
   "source": [
    "topn = 20\n",
    "print(f'DE top{topn}: {src_field.vocab.itos[0:topn]}\\n')\n",
    "print(f'EN top{topn}: {tgt_field.vocab.itos[0:topn]}\\n')\n",
    "\n",
    "\n",
    "\n",
    "UNK_TOKEN = tgt_field.vocab.stoi['<unk>']\n",
    "PAD_TOKEN = tgt_field.vocab.stoi['<pad>']\n",
    "print(f'<unk>={UNK_TOKEN}, <pad>={PAD_TOKEN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "dl_train, dl_valid, dl_test = BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 (S1, B):\n",
      " tensor([[   2,    2,    2,    2],\n",
      "        [  17,    8,    8,   18],\n",
      "        [4467, 7618,   16,   30],\n",
      "        [  65,    9,    7,  279],\n",
      "        [ 302,   17, 3749,   12],\n",
      "        [   7,   20,    0,  134],\n",
      "        [  17,  248,  288,    0],\n",
      "        [ 115, 1142,   12,    9],\n",
      "        [   9, 2610,  172,   35],\n",
      "        [  35,   29,  195,    5],\n",
      "        [  44,    4,   55,  244],\n",
      "        [  22,    3,   91,   17],\n",
      "        [ 205,    1,    4, 2928],\n",
      "        [  17,    1,    3, 7491],\n",
      "        [ 995,    1,    1,    4],\n",
      "        [   0,    1,    1,    3],\n",
      "        [   4,    1,    1,    1],\n",
      "        [   3,    1,    1,    1]]) torch.Size([18, 4])\n",
      "x0_len (B,):\n",
      " tensor([18, 12, 14, 16]) torch.Size([4])\n",
      "y0 (S2, B)):\n",
      " tensor([[   2,    2,    2,    2],\n",
      "        [   7,    4,    4,   16],\n",
      "        [ 178,  523,   14,   30],\n",
      "        [  42,   36,    6,   17],\n",
      "        [ 210,    6,  909,  254],\n",
      "        [  63,    7,   78,   18],\n",
      "        [ 527,  240,  352,  247],\n",
      "        [  20,   12,   78, 3078],\n",
      "        [   7,   44,   44,   15],\n",
      "        [ 116,  316,  198,   28],\n",
      "        [  28,    5,    8,   82],\n",
      "        [  46,    3,    7, 1692],\n",
      "        [  12,    1,   88,    7],\n",
      "        [ 155,    1,    5,    0],\n",
      "        [ 758,    1,    3,    5],\n",
      "        [  27,    1,    1,    3],\n",
      "        [1069,    1,    1,    1],\n",
      "        [  75,    1,    1,    1],\n",
      "        [   5,    1,    1,    1],\n",
      "        [   3,    1,    1,    1]]) torch.Size([20, 4])\n",
      "y0_len: (B,)\n",
      " tensor([20, 12, 15, 16]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# dataloader returns a Batch object with .src and .trg attributes\n",
    "b0 = next(iter(dl_train))\n",
    "\n",
    "# The .src/.trg attributes contain tuples of sequences and their lengths\n",
    "# Get batches of sequences \n",
    "x0, x0_len = b0.src\n",
    "y0, y0_len =  b0.trg\n",
    "\n",
    "print('x0 (S1, B):\\n', x0, x0.shape)\n",
    "print('x0_len (B,):\\n', x0_len, x0_len.shape)\n",
    "print('y0 (S2, B)):\\n', y0, y0.shape)\n",
    "print('y0_len: (B,)\\n', y0_len, y0_len.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Our `encoder` will start very simple. \n",
    "- Embed each source-language token in the sequence into a dense vector representation\n",
    "- Feed the sequence through an RNN (i like GRU, so we would use 2-layers GRU)\n",
    "- Output the final hidden states from each layer\n",
    "\n",
    "Note that we also return the intermediate hidden states from the last layer. We'll need these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, h_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # PyTorch multilayer GRU RNN\n",
    "        self.rnn = nn.GRU(embedding_dim, h_dim, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "        # Encoder has no output layer; we just return hidden states.\n",
    "        \n",
    "    def forward(self, x, **kw):\n",
    "        # x shape: (S, B) Note batch dim is not first!\n",
    "        S, B = x.shape\n",
    "        embedded = self.embedding(x) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # GRU first  output: output of each token (of the last layer) (S, B, H)\n",
    "        # GRU second output: last hidden state from each layer (L, B, H)\n",
    "        y, ht = self.rnn(embedded)\n",
    "        return y, ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sainity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (S1, B, H): torch.Size([18, 4, 64])\n",
      "ht (L, B, H): torch.Size([2, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "EMB_DIM = 128\n",
    "HID_DIM = 64\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "enc = Seq2SeqEncoder(V_src, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "y, ht = enc(x0)\n",
    "print(f'y (S1, B, H): {y.shape}')\n",
    "print(f'ht (L, B, H): {ht.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note again that the output dim is [x,btz,hidden], where x changes?\n",
    "\n",
    "that's because of the bucket iterator that each time aligned to the maximum lenth input\n",
    "\n",
    "Now for the `decoder`. We need to:\n",
    "- Embed target-language tokens\n",
    "- Apply RNN to the sequence, with initial hidden state from encoder\n",
    "- Output is projection of intermediate hidden states from last layer to target-language tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, h_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # PyTorch multilayer GRU RNN\n",
    "        self.rnn = nn.GRU(embedding_dim, h_dim, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "        # Output layer, note the output dimension!\n",
    "        self.out_fc = nn.Linear(h_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, context, **kw):\n",
    "        # x shape: (S, B) the target-language sequence\n",
    "        # context: (L, B, H) the last hidden state from the encoder\n",
    "        S, B = x.shape\n",
    "        embedded = self.embedding(x) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Note initial hidden state is the input context vector\n",
    "        # h:  all out states from last layer (S, B, H)\n",
    "        # ht: last hidden state from each layer (L, B, H)\n",
    "        y, ht = self.rnn(embedded, context)\n",
    "        \n",
    "        # Project H back to the vocab size V, to get a score per word\n",
    "        out = self.out_fc(y)\n",
    "        \n",
    "        # Out shapes: (S, B, V) and (L, B, H)\n",
    "        return out, ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the decoder with the corresponding batch of English sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat (S2, B, V_tgt): torch.Size([20, 4, 5893])\n"
     ]
    }
   ],
   "source": [
    "dec = Seq2SeqDecoder(V_tgt, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "yhat, _ = dec(y0, ht) # note possibly different S\n",
    "print(f'yhat (S2, B, V_tgt): {yhat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll implement a `Seq2Seq` class to handle the combined forward pass through both the encoder and decoder.\n",
    "\n",
    "We saw that we need to input the target sequence to the decoder.\n",
    "That's fine for training.\n",
    "But what do we do for inference, when we obviously don't know the target sequence?\n",
    "\n",
    "During the forward pass, we'll take a source and target sequence and:\n",
    "- **source**: Forward the source sequence through the encoder, obtaining the final hidden state.\n",
    "- **target**: For the decoder, we'll either do:\n",
    "    - **Teacher forcing**: Feed the decoder tokens from the target sequence one by one.\n",
    "    - **Greedy prediction**: Use the decoder's output to predict the most likely next token, and feed it.\n",
    "\n",
    "The greedy prediction approach is what we'll use for inference. For training we'll use **both**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teacher forcing:\n",
    "<center><img src=\"resources/seq2seq.svg\" width=\"1100\"></center>\n",
    "\n",
    "Greedy prediction:\n",
    "<center><img src=\"resources/seq2seq_predict.svg\" width=\"1100\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that obviously in both cases the target sequence is still used to compute the loss.\n",
    "\n",
    "Teacher forcing can speed up training, but it can hurt generalization. We'll take a probabilistic approach:\n",
    "- Use teacher forcing with some probability $p_\\mathrm{tf}$.\n",
    "- Start $p_\\mathrm{tf}$ high and decrease each epoch.\n",
    "- For inference, set $p_\\mathrm{tf}=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Seq2SeqEncoder, decoder: Seq2SeqDecoder):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "    \n",
    "    def forward(self, x_src, x_tgt, p_tf=0, **kw):\n",
    "        # input shapes: (S1, B), (S2, B)\n",
    "        # p_tf: probaility of teacher-forcing decoder input. Zero means greedy-search prediction.\n",
    "        S2, B = x_tgt.shape\n",
    "        \n",
    "        # Forward pass through encoder (entire source seq together)\n",
    "        # context is (L, B, H)\n",
    "        enc_h, context = self.enc(x_src, **kw)\n",
    "        \n",
    "        # First input is first target token\n",
    "        dec_input = x_tgt[[0], :] # (1, B)\n",
    "        # Loop over tokens in target sequence and feed them to the decoder\n",
    "        dec_outputs = []\n",
    "        for t in range(1, S2):\n",
    "            # Feed the decoder sequences of length 1 & save new context\n",
    "            # dec_output is (1, B, V)\n",
    "            # Note: enc_h is for Part 2, ignored by Part 1's decoder\n",
    "            dec_output, context = self.dec(dec_input, context, enc_h=enc_h, **kw)\n",
    "            dec_outputs.append(dec_output)\n",
    "            \n",
    "            # For next input, take either:\n",
    "            # - next target token (AKA \"teacher forcing\"), with proba p_tf\n",
    "            # - highest scoring output (greedy prediction of next token), with proba 1-p_tf\n",
    "            if p_tf > torch.rand(1).item():\n",
    "                dec_input = x_tgt[[t], :] # (1, B)\n",
    "            else:\n",
    "                dec_input = torch.argmax(dec_output, dim=2) # (1,B,V) -> (1, B)\n",
    "            \n",
    "        # Stack decoder outputs from all timesteps\n",
    "        y_hat = torch.cat(dec_outputs, dim=0) # (S-1)x(1,B,V) -> (S-1,B,V)\n",
    "        \n",
    "        # Output shape: (S-1, B, V)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and simple test of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: (S-1, B, V_tgt) = (19, 4, 5893)\n"
     ]
    }
   ],
   "source": [
    "seq2seq_model = Seq2Seq(enc, dec)\n",
    "yhat = seq2seq_model(x0, y0)\n",
    "print('y_hat: (S-1, B, V_tgt) =', tuple(yhat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation\n",
    "\n",
    "The training logic is fairly standard, with only a few nuances:\n",
    "- Notice that the output sequence is shorter than the ground truth due to the `<sos>` token\n",
    "- Clip gradient norm to prevent high grads and stabilize training\n",
    "- When computing the loss, we'll use **Cross Entropy**, but ignore the `<pad>` tokens\n",
    "\n",
    "The following code trains the model for a **single epoch**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, dl_train, optimizer, loss_fn, p_tf=1., clip_grad=1., max_batches=None):\n",
    "    losses = []\n",
    "    with tqdm.tqdm(total=(max_batches if max_batches else len(dl_train)), file=sys.stdout) as pbar:\n",
    "        for idx_batch, batch in enumerate(dl_train, start=1):\n",
    "            x, x_len = batch.src\n",
    "            y, y_len =  batch.trg\n",
    "\n",
    "            # Forward pass: encoder and decoder\n",
    "            # Output y_hat is the translated sequence\n",
    "            y_hat = model(x, y, p_tf, src_len=x_len)\n",
    "            S, B, V = y_hat.shape\n",
    "\n",
    "            # y[:,i] is <sos>, w_1, w_2, ..., w_k, <eos>, <pad>, ...\n",
    "            # y_hat is   w_1', w_2', ..., w_k', <eos>', <pad>', ...\n",
    "            # based on the above, get ground truth y\n",
    "            y_gt = y[1:, :].reshape(S*B)  # drop <sos>\n",
    "            y_hat = y_hat.reshape(S*B, V)\n",
    "\n",
    "            # Calculate loss compared to ground truth y\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_hat, y_gt)\n",
    "            loss.backward()\n",
    "\n",
    "            # Prevent large gradients\n",
    "            if clip_grad > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            pbar.update(); pbar.set_description(f'train loss={losses[-1]:.3f}')\n",
    "            if max_batches and idx_batch >= max_batches:\n",
    "                break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In evaluation mode, we do roughly the same, but make sure to not use teacher forcing.\n",
    "\n",
    "Accuracy migh be a little tricky  in the context of machine translation.\n",
    "\n",
    "we could translate the sentence \"ילד קטן הלך לגן\"<br>\n",
    "as \"There is a little kid that went to the daycare\"<br>\n",
    "or \"A little kid went to daycare\"<br>\n",
    "or \"A small child went to the kindergarden\"<br>\n",
    "\n",
    "and all are valid, so if we use the first one as refrence, how would we know if our model preformed well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu score\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stands for bilingual evaluation understudy.\n",
    "\n",
    "the simple sentence bleu score is given by a unigram precision\n",
    "$$\n",
    "\\mathcal{P} = \\frac{M}{w_t}\n",
    "$$\n",
    "where $M$ is the apearances of words in our prediction that exist in the ground truth, and $w_t$ is the prediction lenght.\n",
    "\n",
    "note that there could be more then one ground truth, and sometimes we use more then one as reference\n",
    "\n",
    "let's demostrate with an example and modify a bit:<br>\n",
    "Candidate prediction: `the the the the the the the`<br>\n",
    "Reference 1: `the cat is on the mat`<br>\n",
    "Reference 2: `there is a cat on the mat` <br>\n",
    "\n",
    "now the unigram bleu score would be $\\frac{7}{7} = 1$<br>\n",
    "\n",
    "the common modification is to use $M_{max}$ means the maximum from all the refrences of a word.<br>\n",
    "in our case the word `the` apear maximum twice and the new score would be $\\frac{2}{7}$<br>\n",
    "\n",
    "\n",
    "another common way to measure Bleu score is on bigrams (remember? 2-grams!)<br>\n",
    "\n",
    "in a similar way, we would count all bigrams in our prediction, and for each bigram, we would use the `count_clip`, the same as in unigram (if the words `the the` apeare 6 times, but 0 times in the refrences and so we clip it to 0), then we sum all cout_clip of bigram and get:\n",
    "\n",
    "$$\n",
    "\\mathcal{P} = \\frac{\\sum_{bigrams}count clip}{w_t}\n",
    "$$\n",
    "\n",
    "\n",
    "another score for NLP tasks is **LEPOR** (Length Penalty, Precision, n-gram Position difference Penalty and Recall)<br>\n",
    "this score is complex, with tuned parameters and we're not going to use it, but you can read more in [here](https://arxiv.org/ftp/arxiv/papers/1703/1703.08748.pdf)\n",
    "(it is actually a masters work from 2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now we will use a metric called BLEU score.\n",
    "Here we just use a function from the `nltk` package to calculate it, as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def eval_seq2seq(model, dl_test):\n",
    "    accuracies = []\n",
    "    bleus = []\n",
    "    with tqdm.tqdm(total=len(dl_test), file=sys.stdout) as pbar:\n",
    "        for idx_batch, batch in enumerate(dl_test):\n",
    "            x, x_len = batch.src\n",
    "            y, y_len =  batch.trg\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Note: no teacher forcing in eval\n",
    "                y_hat = model(x, y, p_tf=0, src_len=x_len)\n",
    "\n",
    "            S, B, V = y_hat.shape\n",
    "\n",
    "            y_gt = y[1:, :] # drop <sos>\n",
    "            y_hat = torch.argmax(y_hat, dim=2) # greedy-sample (S, B, V) -> (S,B)\n",
    "\n",
    "            # Compare prediction to ground truth\n",
    "            accuracies.append(torch.sum(y_gt == y_hat) / float(S))\n",
    "            bleus.append(np.mean([\n",
    "                sentence_bleu([y_gt[:, i]], y_hat[:, i]) for i in range(B)\n",
    "            ]))\n",
    "\n",
    "            pbar.update(); pbar.set_description(f'eval acc={accuracies[-1]}')\n",
    "    return accuracies, bleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EPOCH 1/2, p_tf=1.00 ===\n",
      "train loss=3.682: 100%|██████████| 454/454 [03:48<00:00,  1.98it/s]\n",
      "eval acc=4.333333492279053: 100%|██████████| 16/16 [00:02<00:00,  5.95it/s] \n",
      "=== EPOCH 2/2, p_tf=0.95 ===\n",
      "train loss=3.479: 100%|██████████| 454/454 [03:42<00:00,  2.04it/s]\n",
      "eval acc=5.4242424964904785: 100%|██████████| 16/16 [00:02<00:00,  6.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use small model and dataset so that training is fast, just an example\n",
    "BATCH_SIZE = 64\n",
    "EMB_DIM = 64\n",
    "HID_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "GRAD_CLIP = 1.\n",
    "EPOCHS = 2 #20\n",
    "BATCHES_PER_EPOCH= 50 #454\n",
    "\n",
    "dl_train, dl_valid, dl_test = BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "enc = Seq2SeqEncoder(V_src, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "dec = Seq2SeqDecoder(V_tgt, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "seq2seq_model = Seq2Seq(enc, dec)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(seq2seq_model.parameters(), lr=1e-2)\n",
    "\n",
    "# Note: We don't compute loss from padding tokens\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "bleus = []\n",
    "for idx_epoch in range(EPOCHS):\n",
    "    # Linearly decay amount of teacher forcing for the first 20 epochs (example)\n",
    "    p_tf = 1 - min((idx_epoch / 20), 1)\n",
    "    \n",
    "    print(f'=== EPOCH {idx_epoch+1}/{EPOCHS}, p_tf={p_tf:.2f} ===')\n",
    "    losses += train_seq2seq(seq2seq_model, dl_train, optimizer, loss_fn, p_tf, GRAD_CLIP)#, BATCHES_PER_EPOCH)\n",
    "    acc,bleu = eval_seq2seq(seq2seq_model, dl_valid)\n",
    "    accuracies += acc\n",
    "    bleus += bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "ax[0].plot(losses); ax[0].set_title('train loss'); ax[0].set_xlabel('iteration'); ax[0].grid(True)\n",
    "ax[1].plot(accuracies); ax[1].set_title('eval accuracy'); ax[1].set_xlabel('iteration'); ax[1].grid(True)\n",
    "#ax[2].plot(bleus); ax[2].set_title('Bleu score'); ax[2].set_xlabel('iteration'); ax[2].grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to train for a few days on multiple GPUs to get descent results...\n",
    "Note that using **pre trained embeding** can accelerate the process by much!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What information is captured by the final encoder hidden state when we train such a model end-to-end?\n",
    "\n",
    "Intuitively, we expect it to contain the \"meaning\" of the **entire** sentence, so that the decoder can translate it based on this alone.\n",
    "\n",
    "Sutskever et al. demonstrate this by applying a 2D PCA to the last encoder hidden state:\n",
    "\n",
    "<center><img src=\"resources/sutskever2014_pca.png\", width=\"1400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that sentences with similar meanings appear to be clustered together, i.e. they have a similar representation.\n",
    "\n",
    "isn't that great?<br>\n",
    "well, there are also limitation.<br>\n",
    "In this architecture, the encoder's last hidden state must encode all the information the decoder needs for translation.\n",
    "It must represent the entire meaning of the source sentence, and can't encode local information and lak of temporal knowledge (we do not use the intermidiate hiddens)\n",
    "\n",
    "Can we somehow use this local info to help the decoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention for Alignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll add attention to our model in order to use the encoder's intermediate information. This will allow our decoder to **focus** on different parts of the **source** sequence while it's generating the target sequence.\n",
    "\n",
    "The approach here is based on the classic paper \"Neural Machine Translation by Jointly Learning to Align and Translate\" by [Bahdanau et al.](http://arxiv.org/abs/1409.0473) (ICLR, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we use attention to allow the decoder to focus on different parts on the input?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- We'll treat the encoder's outputs (last layer's hidden states) as both keys and values.\n",
    "- We'll use the decoder's hidden state as a query at each time step.\n",
    "- The attention output will therefore be a weighed average of the encoder outputs most matching the current decoder state.\n",
    "- The output will be treated as a **context**, and concatenated to the decoder input in the next time step.\n",
    "\n",
    "<center><img src=\"resources/seq2seq_attention.svg\" width=1000></center>\n",
    "Note that with this approach, the encoder's last hidden state no longer needs to convey the entire meaning because decoder can look at different parts of the source sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, lets first implement an additive attention block using a 2-layer MLP.\n",
    "\n",
    "As we saw previously, in this type of attention the similarity (\"energy\") function is \n",
    "\n",
    "$$\n",
    "e(\\vec{k},\\vec{q}) = \\vectr{w} \\tanh(\\mat{W}_k\\vec{k} + \\mat{W}_q\\vec{q}),\n",
    "$$\n",
    "\n",
    "where $\\mat{W}_k\\in\\set{R}^{h\\times d_k}$, $\\mat{W}_q\\in\\set{R}^{h\\times d_q}$ and $\\vec{w}\\in\\set{R}^{h}$ are trainable parameters.\n",
    "\n",
    "An important detail is that since we're using the encoder outputs as key/values, and some outputs correspond to `<pad>` tokens, we need a way to tell our attention block to ignore these outputs (give them zero attention weight).\n",
    "\n",
    "We'll add a `seq_len` argument to our attention which should specify the length of the sequence without padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "class MLPAttention(nn.Module):\n",
    "    def __init__(self, q_dim, k_dim, v_dim, h_dim):\n",
    "        super().__init__()\n",
    "        self.wk = nn.Linear(k_dim, h_dim, bias=False)\n",
    "        self.wq = nn.Linear(q_dim, h_dim, bias=False)\n",
    "        self.w  = nn.Linear(h_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, seq_len: Tensor=None):\n",
    "        # q: Queries tensor of shape (B, Q, q_dim)\n",
    "        # k: Keys tensor of shape (B, K, k_dim)\n",
    "        # v: Values tensor of shape (B, K, v_dim)\n",
    "        # seq_len: Sequence lengths tensor of shape (B,). Specifies how many key/values to use in each example.\n",
    "        \n",
    "        # Project keys to hidden dimension\n",
    "        # (B, K, k_dim) -> (B, K, h_dim) -> (B, 1, K, h_dim)\n",
    "        wk_k = self.wk(k).unsqueeze(1)\n",
    "        \n",
    "        # Project queries to hidden dimension\n",
    "        # (B, Q, q_dim)  -> (B, Q, h_dim)  -> (B, Q, 1, h_dim)\n",
    "        wq_q = self.wq(q).unsqueeze(2)\n",
    "        \n",
    "        # First layer of MLP: Use broadcast-addition to combine, then apply nonlinearity\n",
    "        # (B, Q, K, h_dim)\n",
    "        z1 = torch.tanh(wq_q + wk_k)\n",
    "        \n",
    "        # Second layer of MLP\n",
    "        # (B, Q, K, h_dim) -> (B, Q, K, 1) -> (B, Q, K)\n",
    "        z2 = self.w(z1).squeeze(dim=-1)\n",
    "        \n",
    "        # Mask z2 before applying softmax: only seq_len keys are non-padding in each of the B samples\n",
    "        if seq_len is not None:\n",
    "            B, Q, K = z2.shape\n",
    "            idx = torch.arange(K).expand_as(z2)    # (B,Q,K) containing indices 0..K-1\n",
    "            mask = idx >= seq_len.reshape(B, 1, 1) # mask selects indices greater than seq_len\n",
    "            z2[mask] = float('-inf')               # set selected to -inf to prevent influence on softmax\n",
    "        \n",
    "        # Apply softmax on last dimension to get attention weights, per query\n",
    "        a = torch.softmax(z2, dim=-1)\n",
    "        \n",
    "        # Apply the attention weights to the values, per query\n",
    "        # (B, Q, K) * (B, K, v_dim) -> (B, Q, v_dim)\n",
    "        return torch.bmm(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q (B, Q, q_dim) = (2, 1, 3):\n",
      " tensor([[[1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.]]])\n",
      "k (B, K, k_dim) = (2, 4, 2):\n",
      " tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n",
      "v (B, K, v_dim) = (2, 4, 5):\n",
      " tensor([[[ 0.,  4.,  8., 12., 16.],\n",
      "         [ 1.,  5.,  9., 13., 17.],\n",
      "         [ 2.,  6., 10., 14., 18.],\n",
      "         [ 3.,  7., 11., 15., 19.]],\n",
      "\n",
      "        [[20., 24., 28., 32., 36.],\n",
      "         [21., 25., 29., 33., 37.],\n",
      "         [22., 26., 30., 34., 38.],\n",
      "         [23., 27., 31., 35., 39.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create one query of dim 3 (but in a batch of 2)\n",
    "q = torch.ones((2, 1, 3), dtype=torch.float)\n",
    "print(f'q (B, Q, q_dim) = {tuple(q.shape)}:\\n', q)\n",
    "\n",
    "# Create 4 key-value pairs\n",
    "k = torch.ones(2, 4, 2, dtype=torch.float)\n",
    "print(f'k (B, K, k_dim) = {tuple(k.shape)}:\\n', k)\n",
    "v = torch.arange(40, dtype=torch.float).reshape(2, 5, 4).transpose(1, 2)\n",
    "print(f'v (B, K, v_dim) = {tuple(v.shape)}:\\n', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000]],\n",
      "\n",
      "        [[21.5000, 25.5000, 29.5000, 33.5000, 37.5000]]],\n",
      "       grad_fn=<BmmBackward0>)\n",
      "(B, Q, v_dim) = torch.Size([2, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "attn = MLPAttention(q_dim=3, k_dim=2, v_dim=5, h_dim=100)\n",
    "y = attn(q, k, v, seq_len=None)\n",
    "print(y)\n",
    "print(f'(B, Q, v_dim) = {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is a sequence of length 1 because we had one query.\n",
    "\n",
    "Notice that the result is an average of the values, since all keys are equally similar to the query.\n",
    "\n",
    "Let's try with `seq_len=1`, i.e. only the first token in the input is considered valid (not padding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  4.,  8., 12., 16.]],\n",
       "\n",
       "        [[20., 24., 28., 32., 36.]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v, seq_len=torch.tensor([1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update out model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to add the attention block to our model.\n",
    "\n",
    "To do this, we only need to modify our `Seq2SeqDecoder` class. The rest can stay the same.\n",
    "\n",
    "The decoder's `forward` method now needs to also receive all the encoder outputs (`enc_h`) and the length (without padding) of the source sequence (`src_len`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoderAttn(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, h_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The Q, K, V dims are all h_dim because of how we parametrized encoder and decoder\n",
    "        self.attn = MLPAttention(h_dim, h_dim, h_dim, h_dim)\n",
    "        \n",
    "        # Note: GRU input dim now includes both embedding and attention output!\n",
    "        self.rnn = nn.GRU(embedding_dim + h_dim*num_layers, h_dim, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "        self.out_fc = nn.Linear(h_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h_prev, enc_h, src_len, **kw):\n",
    "        # x shape: (S, B)\n",
    "        # h_prev:  (L, B, H) the initial/previous hidden state (L queries for attention)\n",
    "        # enc_h:   (S', B, H) all outputs from encoder** (S' key-value pairs for attention)\n",
    "        # src_len: (B,) the length without padding of the encoder's sequence\n",
    "        S, B = x.shape\n",
    "        embedded = self.embedding(x) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Apply attention: Query is prev hidden state; key/vals are enc outputs without positions of padding\n",
    "        q  = h_prev.transpose(0, 1) # (B, L, H)\n",
    "        kv = enc_h.transpose(0, 1)  # (B, S, H)\n",
    "        a  = self.attn(q, kv, kv, seq_len=src_len)  # (B, L, H)\n",
    "        \n",
    "        # Create RNN input by concatenating attention-based context with the embedded inputs\n",
    "        # Note that when used with Seq2Seq we have S=1 so the expand is a no-op\n",
    "        # (B, L, H) -> (1, B, L*H) -> (S, B, L*H)\n",
    "        a = a.reshape(1, B, -1).expand(S, -1, -1)\n",
    "        rnn_input = torch.cat((embedded, a), dim=2) # (S, B, E + L*H)\n",
    "        \n",
    "        # h:  (S, B, H)\n",
    "        # ht: (L, B, H)\n",
    "        h, ht = self.rnn(rnn_input, h_prev)\n",
    "        \n",
    "        # Project H back to the vocab size V, to get a score per word\n",
    "        out = self.out_fc(h)\n",
    "        \n",
    "        # Out shapes: (S, B, V) and (L, B, H)\n",
    "        return out, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EPOCH 1/40, p_tf=1.00 ===\n",
      "train loss=4.053: 100%|██████████| 454/454 [11:23<00:00,  1.51s/it]\n",
      "eval acc=4.303030490875244: 100%|██████████| 16/16 [00:05<00:00,  3.02it/s]\n",
      "=== EPOCH 2/40, p_tf=0.95 ===\n",
      "train loss=3.749: 100%|██████████| 454/454 [11:19<00:00,  1.50s/it]\n",
      "eval acc=4.848484992980957: 100%|██████████| 16/16 [00:05<00:00,  3.02it/s] \n",
      "=== EPOCH 3/40, p_tf=0.90 ===\n",
      "train loss=3.737: 100%|██████████| 454/454 [11:23<00:00,  1.51s/it]\n",
      "eval acc=5.5757575035095215: 100%|██████████| 16/16 [00:05<00:00,  3.01it/s]\n",
      "=== EPOCH 4/40, p_tf=0.85 ===\n",
      "train loss=3.630: 100%|██████████| 454/454 [11:21<00:00,  1.50s/it]\n",
      "eval acc=4.696969509124756: 100%|██████████| 16/16 [00:05<00:00,  3.06it/s] \n",
      "=== EPOCH 5/40, p_tf=0.80 ===\n",
      "train loss=3.724: 100%|██████████| 454/454 [11:34<00:00,  1.53s/it]\n",
      "eval acc=5.484848499298096: 100%|██████████| 16/16 [00:05<00:00,  3.07it/s] \n",
      "=== EPOCH 6/40, p_tf=0.75 ===\n",
      "train loss=3.711: 100%|██████████| 454/454 [11:56<00:00,  1.58s/it]\n",
      "eval acc=5.151515007019043: 100%|██████████| 16/16 [00:05<00:00,  2.78it/s] \n",
      "=== EPOCH 7/40, p_tf=0.70 ===\n",
      "train loss=4.171: 100%|██████████| 454/454 [12:35<00:00,  1.66s/it]\n",
      "eval acc=5.545454502105713: 100%|██████████| 16/16 [00:05<00:00,  2.77it/s] \n",
      "=== EPOCH 8/40, p_tf=0.65 ===\n",
      "train loss=3.656: 100%|██████████| 454/454 [16:32<00:00,  2.19s/it]\n",
      "eval acc=5.303030490875244: 100%|██████████| 16/16 [00:06<00:00,  2.55it/s] \n",
      "=== EPOCH 9/40, p_tf=0.60 ===\n",
      "train loss=3.774: 100%|██████████| 454/454 [13:29<00:00,  1.78s/it]\n",
      "eval acc=5.39393949508667: 100%|██████████| 16/16 [00:13<00:00,  1.15it/s]  \n",
      "=== EPOCH 10/40, p_tf=0.55 ===\n",
      "train loss=4.011: 100%|██████████| 454/454 [15:04<00:00,  1.99s/it]\n",
      "eval acc=6.030303001403809: 100%|██████████| 16/16 [00:06<00:00,  2.58it/s] \n",
      "=== EPOCH 11/40, p_tf=0.50 ===\n",
      "train loss=3.607:  50%|████▉     | 226/454 [06:43<06:42,  1.76s/it]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "\n",
    "dl_train, dl_valid, dl_test = BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "enc = Seq2SeqEncoder(V_src, EMB_DIM*2, NUM_LAYERS, HID_DIM*2)\n",
    "dec = Seq2SeqDecoderAttn(V_tgt, EMB_DIM*2, NUM_LAYERS, HID_DIM*2)\n",
    "seq2seq_model = Seq2Seq(enc, dec)\n",
    "\n",
    "optimizer = torch.optim.Adam(seq2seq_model.parameters(), lr=1e-2)\n",
    "\n",
    "# Note: We don't compute loss from padding tokens!\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "bleus = []\n",
    "for idx_epoch in range(EPOCHS):\n",
    "    # Linearly decay amount of teacher forcing fro the first 10 epochs (example)\n",
    "    p_tf = 1 - min((idx_epoch / 20), 1)\n",
    "    \n",
    "    print(f'=== EPOCH {idx_epoch+1}/{EPOCHS}, p_tf={p_tf:.2f} ===')\n",
    "    losses += train_seq2seq(seq2seq_model, dl_train, optimizer, loss_fn, p_tf, GRAD_CLIP)#, BATCHES_PER_EPOCH)\n",
    "    acc, bleu = eval_seq2seq(seq2seq_model, dl_valid)\n",
    "    accuracies += acc\n",
    "    bleus += bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(bleus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "ax[0].plot(losses); ax[0].set_title('train loss'); ax[0].set_xlabel('iteration'); ax[0].grid(True)\n",
    "ax[1].plot(accuracies); ax[1].set_title('eval accuracy'); ax[1].set_xlabel('iteration'); ax[1].grid(True)\n",
    "#ax[2].plot(bleus); ax[2].set_title('Bleu score'); ax[2].set_xlabel('iteration'); ax[2].grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the attention layer learning?\n",
    "\n",
    "It's learning to predict the alignment between words in the source (English) and target (French) sentences.\n",
    "\n",
    "<center><img src=\"resources/bahdanau2015-annotated.png\", width=\"1200\"></center>\n",
    "\n",
    "The figure shows the attention weights applied by a trained model to every word in the source sequence (English) when generating the shown target sequence (French).\n",
    "We can see how the trained attention allows the decoder to look forward and back, for example when generating the translation of \"European Economic Area\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the attention mechanisem was yet another method, since implement it and applying it, did not show sagnificant improvements.<br>\n",
    "And then, Google came to the playground, an as they usually do, they take it to the next level.<br>\n",
    "\n",
    "In the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) from 2017, they presented the transformer model, that changed the NLP word for good.\n",
    "\n",
    "before we dive into the model, let's talk about the size, cause size do matter:\n",
    "original transformer came in 2 sizes: `base transformer` with 65M params, and `Big transformer` with 214M params.\n",
    "\n",
    "what happened next? i call it **The moor's law of parameters**:\n",
    "<center><img src=\"resources/NLP_model_size-1-625x266.png\", width=\"500\"></center>\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"resources/number-of-model-parameters-from-Elmo-to-Turing-NLG.png\", width=\"1200\"></center>\n",
    "<center><img src=\"resources/number-of-model-parameters-until-gpt-3.png\", width=\"1200\"></center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've already learned the attention mechanisem, we will focus on the 2 inovative components that lead to this madness:\n",
    "**Multyhead Attention** and **Positional encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multy head attention\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is pretty simple for us, the attention experts ;)\n",
    "\n",
    "as we could have one attention mechanisem, we can use more then one in parallel, and the output would be simply concatination of all the attention heads.\n",
    "\n",
    "<center><img src=\"resources/multheadatt.png\", width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # \"bias=False\" is optional, but for the projection we learned, there is no teoretical justification to use bias\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation of the paper if you would like....\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, 3*Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1) #[Batch, Head, SeqLen, Dims]\n",
    "        \n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim) #concatination all heads\n",
    "        o = self.o_proj(values)\n",
    "        \n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len,input_dim, embed_dim = 20,10,10\n",
    "x = torch.randn(4,seq_len,input_dim)\n",
    "\n",
    "MhA= MultiheadAttention(input_dim, embed_dim, embed_dim//4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,a = MhA(x,return_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MHA output: {out.shape}, attn shape: {a.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we've created output as batch x heads*hidden x hidden, where the attention is batch x heads x deq_len x deq_len\n",
    "\n",
    "essentially, in the transformer model, we put the output in an MLP that reduce to the representation that we want, while the attention score is the attention of each input with respect to the rest of the inputs, we can say this is a learnable corrolation map, with attention to diffrent things.\n",
    "each head learn diffrent thing, just like diffrent filters in CNNs learn diffrent features of the image.\n",
    "\n",
    "\n",
    "as an example, if we use 2 heads, we can see what other words in the input effect a given input token:\n",
    "<center><img src=\"resources/2head.PNG\", width=\"600\"></center>\n",
    "<center><img src=\"resources/QzaaI.png\", width=\"600\"></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea is, as we can learn with the attention mechanisem long distance dependancies, but each mechanisem can learn one context, like, first order relation.\n",
    "when we have more heads, each head can learn diffrent relations, such as correct syntax, gender refrences and so on...\n",
    "concatinate all the heads allow us to use all of this knowledge together and create a very strogn tool, that in theory generalize knowledge just like our brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untill now, we saw a strong tool for learning, but if a token came as the first word in the sentence, or as last, it didn't change anything.\n",
    "\n",
    "the authors of the paper of the transformer suggent that the position of each token is important too...\n",
    "\n",
    "The positional encoding is an idea that reminds Spectral decomposition, like Fourier series, based just on the positions.\n",
    "\n",
    "The basis is Sin and Cosin functions with different coefficients (2pi, 4pi…)\n",
    "\n",
    "The reason of that is that we want to push each embedding a bit to encode the position, but we also want it to be normalized. One might use just one Sin function will get the same positional encoding for words in positions of the same frequencies of the Sin...\n",
    "\n",
    "in the transformer paper it looks neglectable, but it's actually a very novel idea\n",
    "\n",
    "<center><img src=\"resources/transformer.png\", width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets implement it ourself and try to see how the additional to each encoded word is diffrent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) \n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
    "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
    "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
    "fig.colorbar(pos, ax=ax)\n",
    "ax.set_xlabel(\"Position in sequence\")\n",
    "ax.set_ylabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
    "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with the \"super-position\" between all the sin and cosin functions, each token would get a differnt additional to the encoding!\n",
    "\n",
    "if it's confusing, we can choose some of the functions and zoom in a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn - if it's not part of the requrement file\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12,4))\n",
    "ax = [a for a_list in ax for a in a_list]\n",
    "for i in range(len(ax)):\n",
    "    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n",
    "    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n",
    "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
    "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
    "    ax[i].set_xticks(np.arange(1,17))\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n",
    "    ax[i].set_ylim(-1.2, 1.2)\n",
    "fig.subplots_adjust(hspace=0.8)\n",
    "sns.reset_orig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q: can we use the transformer for a text generation task?\n",
    "\n",
    "a: short answer is yes, but it really depend on the nature of our decoder.\n",
    "\n",
    "in Bert model, B stands for bi-directional, and therfore it's good for tasks that need to see future words.\n",
    "\n",
    "in general, for text generation, we feed the decoder with some context, and can generate text with some probabilities... we will see how :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with pre-trained transformers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer models are Huge, and we've yet to learn about generative models...\n",
    "\n",
    "the model was trained with batches of 512 over multiple GPU's for weeks.. \n",
    "\n",
    "however, some cool company called [huggingface](https://huggingface.co/) is bisically a hub for pretrained complex models that you can use, fine-tune and explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a model called [GPT-2](https://openai.com/blog/better-language-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note is, the model come in diffrent sizes (small, medium, large and X-large).\n",
    "we're going to use a lighter version the small model with 124M params!!!!\n",
    "\n",
    "<center><img src=\"resources/gpt2.PNG\", width=\"800\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're not going to go too much into details\n",
    "but as you can see in the image aboive, GPT2 takes a large transformers (decoders) and stack them together (similar to attention heads, but with the hole decoder)\n",
    "the encoder is fairly similar, again as we didn't talk about VAE's yet, assume that what we do here is decoding some short input, and trying to use the latent representation of it to predict the next word, as for when we don't have more generated words, we use the output of the encoder as an input for the decoder again...\n",
    "\n",
    "\n",
    "we will use a version with corpus that has 768 tokens, but of course you can go up to 10k...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workshops are new, and so to get the pretrained model, first we would install the library of huggingface called transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completes = generator(\"if i had a nickel for everytime a student told me that the homework is too long,\", max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(completes):\n",
    "    print(f'===generated sentence number {i}: ===')\n",
    "    print(' ' + sentence['generated_text']+ '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "even new models have biasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to try and use the model with input and output, as an aplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"I really like to learn\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first output give score to each input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the second output is the decoders output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize attention you can see in [here](https://huggingface.co/exbert/?model=gpt2&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=5&heads=..&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)\n",
    "\n",
    "or, you can add to your own code with this [cool implementation](https://github.com/jessevig/bertviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video By Subject\n",
    "Thanks to Tal Daniel\n",
    "\n",
    "* Deep Learning for Natural Language Processing (NLP) -  <a href=\"https://youtu.be/6D4EWKJgNn0\"> Deep Learning for Natural Language Processing (NLP) </a>\n",
    "    * Attention and the Transformer <a href=\"https://www.youtube.com/watch?v=f01J0Dri-6k&feature=youtu.be\">Practicum: Attention and the Transformer</a>\n",
    "\n",
    "* Recurrent Neural Networks - <a href=\"https://www.youtube.com/watch?v=SEnXr6v2ifU\"> Recurrent Neural Networks | MIT 6.S191 </a>\n",
    "\n",
    "* LSTM & GRU - <a href=\"https://www.youtube.com/watch?v=8HyCNIVRbSU\"> Illustrated Guide to LSTM's and GRU's: A step by step explanation </a>\n",
    "\n",
    "* Transformers - <a href=\"https://www.youtube.com/watch?v=S27pHKBEp30\">LSTM is dead. Long Live Transformers! </a>\n",
    "* BERT - <a href=\"https://www.youtube.com/watch?v=OR0wfP2FD3c\">BERT Explained!</a>\n",
    "* GPT - <a href=\"https://www.youtube.com/watch?v=9ebPNEHRwXU\">GPT Explained!</a>\n",
    "    * [GPT 2](https://jalammar.github.io/illustrated-gpt2/) \n",
    "    * GPT-3 - <a href=\"https://www.youtube.com/watch?v=_x9AwxfjxvE\">OpenAI GPT-3 - Good At Almost Everything!</a>\n",
    "\n",
    "- Transformers- pre trained\n",
    "[Hugging face](https://huggingface.co/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Credits**\n",
    "\n",
    "\n",
    "This tutorial was written by [Moshe Kimhi](https://www.linkedin.com/in/moshekimhi/)<br>\n",
    "the translation and alignment models parts, was written by [Aviv A. Rosenberg](https://avivr.net) as well as insperation for this tutorial.<br>\n",
    "\n",
    "To re-use, please provide attribution and link to the original.\n",
    "\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from the following sources:\n",
    "\n",
    "- K. Xu et al. 2015, http://proceedings.mlr.press/v37/xuc15.html\n",
    "- Sutskever et al. 2014, https://arxiv.org/abs/1409.3215\n",
    "- Bahdanau et al. 2015, http://arxiv.org/abs/1409.0473\n",
    "- Zhang et al., Dive into Deep Learning, 2019\n",
    "- Peter Bloem, http://www.peterbloem.nl/blog/transformers\n",
    "- Chris Olah, https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Ben Trevett, http://bentrevett.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
